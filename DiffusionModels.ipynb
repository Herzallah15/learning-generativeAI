{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3109c05a-8d41-4bf7-8b33-2537f5b03470",
   "metadata": {},
   "source": [
    "# Diffusion Models and the HuggingFace Diffusers Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e50b68e-344a-42b9-8711-3e63b8f83929",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "In the previous notebooks, we explored **Variational Autoencoders (VAEs)**, one of the foundational generative models. While VAEs introduced us to probabilistic latent spaces and the power of learned representations, they have a well-known limitation: they tend to produce blurry outputs and struggle with photorealistic, high-resolution image generation.\n",
    "\n",
    "In this notebook, we introduce **Diffusion Models**, the generative architecture behind state-of-the-art systems like **Stable Diffusion**, **DALL-E 2**, **Midjourney**, and **Imagen**. These models have revolutionized image generation, enabling the creation of stunning, photorealistic images from text descriptions.\n",
    "\n",
    "**What We'll Cover:**\n",
    "\n",
    "1. **The Generative Modeling Landscape**: Where diffusion models fit among VAEs and GANs\n",
    "2. **How Diffusion Models Work**: The forward (noising) and reverse (denoising) processes\n",
    "3. **Why Diffusion Models Excel**: Quality, diversity, and the iterative refinement advantage\n",
    "4. **Text-to-Image Generation**: How conditioning enables controllable generation\n",
    "5. **The HuggingFace Ecosystem**: An overview of the tools we'll use\n",
    "6. **Introduction to the Diffusers Library**: Our gateway to state-of-the-art diffusion models\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aee32d-e4ee-4c3e-84cf-5e484ee3663e",
   "metadata": {},
   "source": [
    "## The Generative Modeling Trilemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc36e246-7414-45f5-8a57-b58dd01a2cc3",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "Before diving into diffusion models, it's helpful to understand where they fit in the broader landscape of generative models. There's a well-known **trilemma** in generative modeling, where models must trade off between three desirable properties:\n",
    "\n",
    "| Property | Description |\n",
    "|----------|-------------|\n",
    "| **Quality** | How realistic and detailed are the generated outputs? |\n",
    "| **Diversity** | Can the model generate a wide variety of different outputs? |\n",
    "| **Speed** | How fast can the model generate new samples? |\n",
    "\n",
    "**Where Different Models Fall:**\n",
    "\n",
    "- **Variational Autoencoders (VAEs)**: Fast and diverse, but lower quality (blurry outputs)\n",
    "- **Generative Adversarial Networks (GANs)**: High quality and fast, but can suffer from mode collapse (limited diversity)\n",
    "- **Diffusion Models**: Exceptional quality and diversity, but slower due to iterative generation\n",
    "\n",
    "The \"impossible\" center of the trilemma, achieving all three simultaneously, remains an active area of research. Recent advances like **Latent Diffusion Models** (used in Stable Diffusion) and distillation techniques are pushing closer to this goal.\n",
    "\n",
    "**Why Diffusion Models Have Become Dominant:**\n",
    "\n",
    "Despite their speed disadvantage, diffusion models have become the go-to choice for image generation because:\n",
    "1. They scale incredibly well to high resolutions\n",
    "2. They produce remarkably diverse outputs without mode collapse\n",
    "3. Their iterative nature allows for controllable generation and guidance\n",
    "4. They're more stable to train than GANs\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813055f-b88c-4087-a31a-f598f7b09551",
   "metadata": {},
   "source": [
    "## How Diffusion Models Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b92cb-1d49-4a77-b61c-05e627365381",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "The core idea behind diffusion models is elegantly simple:\n",
    "\n",
    "> **If we can learn to gradually destroy data by adding noise, can we learn to reverse that process and create data from noise?**\n",
    "\n",
    "This leads to a two-phase framework:\n",
    "\n",
    "**The Forward Process (Encoding / Noising)**\n",
    "\n",
    "Starting with a real image $x_0$, we progressively add Gaussian noise over $T$ timesteps:\n",
    "\n",
    "$$\n",
    "x_0 \\xrightarrow{+\\text{noise}} x_1 \\xrightarrow{+\\text{noise}} x_2 \\xrightarrow{+\\text{noise}} \\cdots \\xrightarrow{+\\text{noise}} x_T \\approx \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "By timestep $T$, the image has been completely corrupted into pure Gaussian noise. This process is **fixed** (not learned) and defined by a **noise schedule**.\n",
    "\n",
    "**The Reverse Process (Decoding / Denoising)**\n",
    "\n",
    "A neural network learns to reverse this process, progressively removing noise:\n",
    "\n",
    "$$\n",
    "x_T \\xrightarrow{-\\text{noise}} x_{T-1} \\xrightarrow{-\\text{noise}} \\cdots \\xrightarrow{-\\text{noise}} x_1 \\xrightarrow{-\\text{noise}} x_0\n",
    "$$\n",
    "\n",
    "Starting from pure noise $x_T \\sim \\mathcal{N}(0, I)$, the model iteratively denoises to produce a clean image.\n",
    "\n",
    "**Key Insight: Same Dimensionality Throughout**\n",
    "\n",
    "Unlike VAEs where the latent space is compressed ($D_z \\ll D_x$), in diffusion models:\n",
    "\n",
    "$$\n",
    "\\text{dim}(x_0) = \\text{dim}(x_1) = \\cdots = \\text{dim}(x_T)\n",
    "$$\n",
    "\n",
    "The \"latent space\" is simply **noised image space** of the same dimension as the output.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58faca5-1256-422c-877e-69f778f39365",
   "metadata": {},
   "source": [
    "## Mathematical Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42694d6-3329-42ef-85c7-7921f8a103ae",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "**The Forward Process $q(x_t | x_{t-1})$**\n",
    "\n",
    "The forward process is a **Markov chain** that gradually adds Gaussian noise according to a variance schedule $\\beta_1, \\beta_2, \\ldots, \\beta_T$:\n",
    "\n",
    "$$\n",
    "q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} \\, x_{t-1}, \\beta_t I)\n",
    "$$\n",
    "\n",
    "This means at each step:\n",
    "- We scale down the previous image by $\\sqrt{1 - \\beta_t}$\n",
    "- We add Gaussian noise with variance $\\beta_t$\n",
    "\n",
    "**Useful Property**: We can sample $x_t$ directly from $x_0$ without iterating through all steps:\n",
    "\n",
    "$$\n",
    "q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} \\, x_0, (1 - \\bar{\\alpha}_t) I)\n",
    "$$\n",
    "\n",
    "where $\\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$.\n",
    "\n",
    "This can be written as:\n",
    "$$\n",
    "x_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d3086-0423-4ae1-a011-26ad77c337d2",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "**The Reverse Process $p_\\theta(x_{t-1} | x_t)$**\n",
    "\n",
    "The reverse process is what we **learn**. We train a neural network to approximate:\n",
    "\n",
    "$$\n",
    "p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))\n",
    "$$\n",
    "\n",
    "The network predicts the mean $\\mu_\\theta$ (and optionally variance $\\Sigma_\\theta$) of the denoised image at each step.\n",
    "\n",
    "**In Practice**: Rather than predicting $\\mu_\\theta$ directly, most implementations train the network to predict the **noise** $\\epsilon_\\theta(x_t, t)$ that was added. The mean can then be computed as:\n",
    "\n",
    "$$\n",
    "\\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right)\n",
    "$$\n",
    "\n",
    "**Training Objective**\n",
    "\n",
    "The model is trained to predict the noise that was added, using a simple MSE loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathbb{E}_{t, x_0, \\epsilon} \\left[ \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $t \\sim \\text{Uniform}(1, T)$ is a random timestep\n",
    "- $\\epsilon \\sim \\mathcal{N}(0, I)$ is the noise added to create $x_t$ from $x_0$\n",
    "- $\\epsilon_\\theta(x_t, t)$ is the network's prediction of that noise\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cf7864-7abd-4996-85ff-21ef91b0a9f9",
   "metadata": {},
   "source": [
    "### Connection to Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bc18d0-2746-4f01-a1b5-1b2062bd45a1",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "Diffusion models can actually be understood through the lens of VAEs:\n",
    "\n",
    "| Aspect | VAE | Diffusion Model |\n",
    "|--------|-----|------------------|\n",
    "| **Encoder** | $q_\\phi(z|x)$ (learned) | $q(x_t|x_0)$ (fixed, adds noise) |\n",
    "| **Latent Space** | Low-dimensional $z$ | Same-dimensional noised images $x_t$ |\n",
    "| **Decoder** | $p_\\theta(x|z)$ (one step) | $p_\\theta(x_{t-1}|x_t)$ ($T$ steps) |\n",
    "| **Training** | ELBO maximization | Simplified noise prediction |\n",
    "\n",
    "Each transition $x_t \\to x_{t-1}$ can be thought of as a mini-VAE, and the full diffusion model chains $T$ of these together.\n",
    "\n",
    "The key difference is that diffusion models:\n",
    "1. Use a **fixed** encoder (just add noise according to a schedule)\n",
    "2. Learn a **sequence** of small denoising steps rather than one big decoding step\n",
    "3. Keep the **full dimensionality** throughout the process\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd6313a-923d-4dfb-8575-67a0e5b2e9de",
   "metadata": {},
   "source": [
    "## The Power of Iterative Denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81189b14-23ba-4bc0-b9d0-420697dcb2c2",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "One of the most powerful features of diffusion models is their **iterative nature**. Unlike VAEs and GANs that generate images in a single forward pass, diffusion models refine their output over many steps.\n",
    "\n",
    "**Adjustable Quality-Speed Tradeoff**\n",
    "\n",
    "The number of denoising steps $T$ is adjustable at inference time:\n",
    "\n",
    "| Fewer Steps | More Steps |\n",
    "|-------------|------------|\n",
    "| Faster generation | Slower generation |\n",
    "| Lower quality | Higher quality |\n",
    "| Good for previews | Good for final outputs |\n",
    "\n",
    "This gives users control over the quality-speed tradeoff based on their needs.\n",
    "\n",
    "**Generation Process**\n",
    "\n",
    "To generate a new image:\n",
    "\n",
    "1. **Sample pure noise**: $x_T \\sim \\mathcal{N}(0, I)$\n",
    "2. **Iteratively denoise**: For $t = T, T-1, \\ldots, 1$:\n",
    "   - Predict noise: $\\hat{\\epsilon} = \\epsilon_\\theta(x_t, t)$\n",
    "   - Compute $x_{t-1}$ using the predicted noise\n",
    "3. **Output**: $x_0$ is the final generated image\n",
    "\n",
    "This is analogous to the VAE generation process:\n",
    "- **VAE**: Sample $z \\sim \\mathcal{N}(0, I)$, decode in one step\n",
    "- **Diffusion**: Sample $x_T \\sim \\mathcal{N}(0, I)$, decode in $T$ steps\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baee210-d7cf-4ccc-84fd-1efdb6870d89",
   "metadata": {},
   "source": [
    "## Conditional Generation: Text-to-Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb4bde2-ff0e-405b-aa07-6d21650b1ff6",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "The iterative nature of diffusion models makes them exceptionally well-suited for **conditional generation**, where we want to guide the output based on some input (like a text prompt).\n",
    "\n",
    "**How Text-to-Image Works**\n",
    "\n",
    "1. **Encode the text prompt**: A text encoder (typically a Transformer) converts the prompt into an embedding vector\n",
    "   \n",
    "   $$\\text{\"A golden retriever playing in the snow\"} \\xrightarrow{\\text{Transformer}} c_{\\text{text}}$$\n",
    "\n",
    "2. **Sample noise**: Start with random Gaussian noise $x_T \\sim \\mathcal{N}(0, I)$\n",
    "\n",
    "3. **Conditioned denoising**: The denoising network takes both the noisy image AND the text embedding:\n",
    "   \n",
    "   $$\\epsilon_\\theta(x_t, t, c_{\\text{text}})$$\n",
    "   \n",
    "   The text conditioning guides each denoising step toward images that match the description.\n",
    "\n",
    "4. **Output**: After $T$ steps, we get an image matching the text prompt\n",
    "\n",
    "**Why This Works So Well**\n",
    "\n",
    "- **Gradual refinement**: The model can make small adjustments at each step to better match the conditioning\n",
    "- **Flexibility**: The same architecture works for text, images, class labels, or any other conditioning\n",
    "- **Guidance scales**: We can control how strongly the output follows the conditioning\n",
    "\n",
    "**Classifier-Free Guidance**\n",
    "\n",
    "Modern text-to-image models use **classifier-free guidance** to strengthen the effect of the text prompt:\n",
    "\n",
    "$$\n",
    "\\hat{\\epsilon} = \\epsilon_\\theta(x_t, t, \\emptyset) + s \\cdot (\\epsilon_\\theta(x_t, t, c) - \\epsilon_\\theta(x_t, t, \\emptyset))\n",
    "$$\n",
    "\n",
    "where $s > 1$ is the guidance scale. Higher values produce images that more strongly match the prompt (but may sacrifice diversity).\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1928bd-3ff9-4140-8e9c-bf22b78ff617",
   "metadata": {},
   "source": [
    "## The HuggingFace Ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c4fd94-f731-497a-944e-3d771cb64cdc",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "Before we start coding, let's understand the tools we'll be using. **HuggingFace** has become the central hub for open-source machine learning, providing:\n",
    "\n",
    "**Core Libraries**\n",
    "\n",
    "| Library | Purpose | We'll Use It For |\n",
    "|---------|---------|------------------|\n",
    "| **ðŸ¤— Transformers** | NLP and beyond | Text encoders for conditioning |\n",
    "| **ðŸ§¨ Diffusers** | Diffusion models | Image generation pipelines |\n",
    "| **ðŸ“¦ Datasets** | Data loading | Training data (if fine-tuning) |\n",
    "| **ðŸš€ Accelerate** | Distributed training | Multi-GPU training |\n",
    "\n",
    "**The Diffusers Library**\n",
    "\n",
    "The `diffusers` library is our main focus. It provides:\n",
    "\n",
    "1. **Pipelines**: High-level APIs for common tasks (text-to-image, image-to-image, inpainting)\n",
    "2. **Models**: Pre-trained UNet, VAE, and text encoder components\n",
    "3. **Schedulers**: Different noise schedules and sampling algorithms\n",
    "4. **Utilities**: Image processing, model loading, and more\n",
    "\n",
    "**Why HuggingFace?**\n",
    "\n",
    "- **Unified API**: Consistent interface across different models\n",
    "- **Model Hub**: Thousands of pre-trained models ready to use\n",
    "- **Active Community**: Regular updates, tutorials, and support\n",
    "- **Interoperability**: Libraries work seamlessly together\n",
    "\n",
    "**Documentation Resources**\n",
    "\n",
    "- **Diffusers Docs**: [huggingface.co/docs/diffusers](https://huggingface.co/docs/diffusers)\n",
    "- **Model Hub**: [huggingface.co/models](https://huggingface.co/models)\n",
    "- **Tutorials**: [huggingface.co/docs/diffusers/tutorials](https://huggingface.co/docs/diffusers/tutorials)\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f2c616-09d2-482f-b346-668a6b566871",
   "metadata": {},
   "source": [
    "## Introduction to Diffusers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472aa2c7-8536-4933-ac51-906069c9ede9",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "The `diffusers` library makes it remarkably easy to use state-of-the-art diffusion models. Let's start with the basics.\n",
    "\n",
    "**Installation**\n",
    "\n",
    "```bash\n",
    "pip install diffusers transformers accelerate torch\n",
    "```\n",
    "\n",
    "**Key Concepts**\n",
    "\n",
    "**1. Pipelines**: End-to-end workflows that combine all components\n",
    "```python\n",
    "from diffusers import StableDiffusionPipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "image = pipe(\"a photo of a cat\").images[0]\n",
    "```\n",
    "\n",
    "**2. Schedulers**: Control the noise schedule and sampling process\n",
    "```python\n",
    "from diffusers import DDPMScheduler, DDIMScheduler, EulerScheduler\n",
    "```\n",
    "\n",
    "**3. Models**: The neural network components\n",
    "```python\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "```\n",
    "\n",
    "**Common Pipelines**\n",
    "\n",
    "| Pipeline | Task | Input â†’ Output |\n",
    "|----------|------|----------------|\n",
    "| `StableDiffusionPipeline` | Text-to-Image | Text â†’ Image |\n",
    "| `StableDiffusionImg2ImgPipeline` | Image-to-Image | Image + Text â†’ Image |\n",
    "| `StableDiffusionInpaintPipeline` | Inpainting | Image + Mask + Text â†’ Image |\n",
    "| `StableDiffusionUpscalePipeline` | Super-resolution | Low-res â†’ High-res |\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d4ce23-a709-4a70-8af7-18dd041194fe",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcee5074-7c50-48c0-9ed8-6e0343764e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# HuggingFace imports\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Check device\n",
    "device = (\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else 'mps' if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba75f6e-3257-41af-8cd2-dcc1d3a4bbc9",
   "metadata": {},
   "source": [
    "## First Diffusion Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0695a0a1-4065-45de-b91a-d879f482be6b",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "Let's generate our first image using Stable Diffusion. We'll start with a simple, clear example.\n",
    "\n",
    "**Note**: Running Stable Diffusion requires a GPU with at least 8GB VRAM. If you're on CPU or have limited memory, we'll also show alternatives.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f47bc-f74c-44c7-b3e0-e8c775c43b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
