{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d12456aa-0d2c-4aab-8dc5-961843e7ec40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x114d6d250>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 4)\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac99b9e-0cc3-4587-8b99-568bf2533e5b",
   "metadata": {},
   "source": [
    "# Variational Autoencoders (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1faae7-cb45-4787-bc85-b0182dd9f099",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "In the previous notebook (`Latent_Variable_Models_and_Autoencoders.ipynb`), we introduced the standard autoencoder, a neural network that learns compressed representations of data through an encoder-decoder architecture. While autoencoders are powerful for dimensionality reduction and feature learning, they have a fundamental limitation: **they are not true generative models**.\n",
    "\n",
    "The problem lies in the latent space structure. In a standard autoencoder:\n",
    "- The encoder maps each input $x$ to a **deterministic** point $z$ in latent space\n",
    "- The latent space has \"holes\", regions where no training data maps to\n",
    "- Sampling random vectors from the latent space often produces meaningless outputs\n",
    "\n",
    "The **Variational Autoencoder (VAE)**, introduced by Kingma and Welling (2014), addresses these limitations by introducing **probability** and **stochasticity** into the autoencoder framework, **transforming** it into a proper generative model.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b340206c-2090-4222-ac71-4e63ddb742d9",
   "metadata": {},
   "source": [
    "## The Natural Image Manifold and Latent Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc30501-b0a2-484e-b696-aa8fd5186653",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "Before diving into the VAE architecture, let's build intuition about what we're trying to learn.\n",
    "\n",
    "**The Pixel Space $\\mathcal{X}$**\n",
    "\n",
    "Consider the space of all possible images. For a $28 \\times 28$ grayscale image, this is $\\mathcal{X} = [0, 1]^{784}$, a 784-dimensional hypercube. However, not all points in this space correspond to \"natural\" or meaningful images. Random noise is technically a valid point in pixel space, but it doesn't represent anything recognizable.\n",
    "\n",
    "**The Natural Image Manifold**\n",
    "\n",
    "Real-world images (faces, digits, cats, etc.) occupy only a tiny subspace of the full pixel space. This subspace is often called the **natural image manifold**,a complex, high-dimensional surface where:\n",
    "- Similar images are close together (cats near cats, dogs near dogs)\n",
    "- The manifold captures the underlying structure that makes images \"natural\"\n",
    "\n",
    "**The Latent Space $\\mathcal{Z}$**\n",
    "\n",
    "The goal of generative models is to learn a **latent space** $\\mathcal{Z}$ that:\n",
    "1. Is lower-dimensional than pixel space ($D_z \\ll D_x$)\n",
    "2. Captures the essential features of the data (\"two eyes,\" \"four legs,\" \"curved stroke\")\n",
    "3. Is **smooth** and **continuous**, nearby points produce similar outputs\n",
    "4. Can be easily sampled from for generation\n",
    "\n",
    "A well-trained VAE learns a latent space where:\n",
    "$$\n",
    "\\text{Simple distribution in } \\mathcal{Z} \\xrightarrow{\\text{Decoder}} \\text{Complex distribution in } \\mathcal{X}\n",
    "$$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bd1157-0437-4b84-85ff-561e780494b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## From Autoencoder to Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c6d2e5-c0a3-4b9c-bb15-14c78180bc3b",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "**The Limitation of Deterministic Autoencoders**\n",
    "\n",
    "Recall that a standard autoencoder performs the following mapping:\n",
    "$$\n",
    "x \\xrightarrow{E_\\phi} z \\xrightarrow{D_\\theta} \\hat{x}\n",
    "$$\n",
    "\n",
    "where the encoder $E_\\phi$ maps each input to a **single, fixed point** in latent space. This deterministic mapping means:\n",
    "- The same input always produces the same latent vector\n",
    "- The latent space has no inherent structure, points are scattered wherever minimizes reconstruction error\n",
    "- We cannot meaningfully sample from the latent space\n",
    "\n",
    "**The VAE Solution: Probabilistic Encoding**\n",
    "\n",
    "The key insight of the VAE is to replace the deterministic encoder with a **probabilistic** one:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\text{Autoencoder: } x \\to z \\quad \\text{vs} \\quad \\text{VAE: } x \\to p(z|x) = \\mathcal{N}(\\mu, \\sigma^2)\n",
    "}\n",
    "$$\n",
    "\n",
    "Instead of encoding to a point, the VAE encoder outputs the **parameters of a probability distribution**:\n",
    "- A mean vector $\\mu \\in \\mathbb{R}^{D_z}$\n",
    "- A variance vector $\\sigma^2 \\in \\mathbb{R}^{D_z}$ (or equivalently, log-variance $\\log \\sigma^2$)\n",
    "\n",
    "The latent vector $z$ is then **sampled** from this distribution:\n",
    "$$\n",
    "z \\sim \\mathcal{N}(\\mu, \\text{diag}(\\sigma^2))\n",
    "$$\n",
    "\n",
    "This simple change has profound consequences:\n",
    "1. **Stochasticity**: The same input can produce different latent vectors (and thus different reconstructions)\n",
    "2. **Structured latent space**: The latent space is regularized to approximate a known distribution\n",
    "3. **Generative capability**: We can sample $z \\sim \\mathcal{N}(0, I)$ and decode to generate new images\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c824609-10d6-4103-a770-022c29c1c70c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mathematical Framework of VAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e854ca2-60e3-4d76-b42a-ad2dd6c1f827",
   "metadata": {},
   "source": [
    "**The Generative Model**\n",
    "\n",
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "The VAE assumes the following generative process for data:\n",
    "\n",
    "1. Sample a latent variable from a **prior** distribution:\n",
    "   $$z \\sim p(z) = \\mathcal{N}(0, I)$$\n",
    "\n",
    "2. Generate data from a **conditional likelihood**:\n",
    "   $$x \\sim p_\\theta(x|z)$$\n",
    "\n",
    "where $\\theta$ are the parameters of the decoder network. The decoder defines $p_\\theta(x|z)$, which for continuous data is often a Gaussian with mean given by the decoder output.\n",
    "\n",
    "**The Inference Problem**\n",
    "\n",
    "Given observed data $x$, we want to infer the latent variable $z$ that generated it. By Bayes' theorem:\n",
    "\n",
    "$$\n",
    "p(z|x) = \\frac{p_\\theta(x|z) \\, p(z)}{p(x)}\n",
    "$$\n",
    "\n",
    "However, the **evidence** $p(x) = \\int p_\\theta(x|z) \\, p(z) \\, dz$ is intractable, it requires integrating over all possible latent vectors.\n",
    "\n",
    "**Variational Inference**\n",
    "\n",
    "Since we cannot compute $p(z|x)$ exactly, we **approximate** it with a simpler distribution $q_\\phi(z|x)$ parameterized by the encoder network:\n",
    "\n",
    "$$\n",
    "q_\\phi(z|x) = \\mathcal{N}(z; \\mu_\\phi(x), \\text{diag}(\\sigma^2_\\phi(x)))\n",
    "$$\n",
    "\n",
    "The encoder neural network outputs $\\mu_\\phi(x)$ and $\\sigma^2_\\phi(x)$ (or $\\log \\sigma^2_\\phi(x)$) for each input $x$.\n",
    "\n",
    "**Goal**: Find parameters $\\phi$ such that $q_\\phi(z|x)$ is close to the true posterior $p(z|x)$.\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5be50-f151-495f-a741-7f9385883ed5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### The KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69063a5-113a-4e27-b56e-f88df35850d8",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "To measure how close $q_\\phi(z|x)$ is to $p(z|x)$, we use the **Kullback-Leibler (KL) divergence**:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(q \\| p) = \\mathbb{E}_{z \\sim q}\\left[ \\log \\frac{q(z)}{p(z)} \\right] = \\int q(z) \\log \\frac{q(z)}{p(z)} \\, dz\n",
    "$$\n",
    "\n",
    "**Key properties of KL divergence:**\n",
    "\n",
    "1. $D_{\\text{KL}}(q \\| p) \\geq 0$ always (non-negative)\n",
    "2. $D_{\\text{KL}}(q \\| p) = 0$ if and only if $q = p$ almost everywhere\n",
    "3. **Asymmetric**: $D_{\\text{KL}}(q \\| p) \\neq D_{\\text{KL}}(p \\| q)$ in general\n",
    "\n",
    "The KL divergence can be interpreted as:\n",
    "- The expected **extra bits** needed to encode samples from $q$ using a code optimized for $p$\n",
    "- A measure of **information loss** when using $q$ to approximate $p$\n",
    "\n",
    "**Intuition**: If $q$ and $p$ are both Gaussians, $D_{\\text{KL}}(q \\| p)$ penalizes differences in both mean and variance.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1762f75-8e5c-442f-915e-e5cb1f45cda2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### The Evidence Lower Bound (ELBO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b40984-6f4f-4dbc-957f-193517e8dc66",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "We want to maximize the log-likelihood of our data $\\log p(x)$. Through a derivation involving Jensen's inequality, we can show:\n",
    "\n",
    "$$\n",
    "\\log p(x) = \\underbrace{\\mathbb{E}_{z \\sim q_\\phi(z|x)}\\left[ \\log p_\\theta(x|z) \\right]}_{\\text{Reconstruction term}} - \\underbrace{D_{\\text{KL}}(q_\\phi(z|x) \\| p(z))}_{\\text{Regularization term}} + \\underbrace{D_{\\text{KL}}(q_\\phi(z|x) \\| p(z|x))}_{\\geq 0}\n",
    "$$\n",
    "\n",
    "Since $D_{\\text{KL}}(q_\\phi(z|x) \\| p(z|x)) \\geq 0$, we have:\n",
    "\n",
    "$$\n",
    "\\log p(x) \\geq \\underbrace{\\mathbb{E}_{z \\sim q_\\phi(z|x)}\\left[ \\log p_\\theta(x|z) \\right] - D_{\\text{KL}}(q_\\phi(z|x) \\| p(z))}_{\\text{ELBO}(x; \\phi, \\theta)}\n",
    "$$\n",
    "\n",
    "This lower bound is called the **Evidence Lower Bound (ELBO)**. Maximizing the ELBO simultaneously:\n",
    "1. Maximizes the data likelihood (generative quality)\n",
    "2. Minimizes $D_{\\text{KL}}(q_\\phi(z|x) \\| p(z|x))$ (inference quality)\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982af6fb-1f34-4d9c-b1d0-12c3870eac4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### The VAE Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b2c7cb-8baa-4fe2-aa67-bf77d8dda066",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "The VAE is trained by **maximizing the ELBO**, or equivalently, **minimizing the negative ELBO**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{VAE}}(x; \\phi, \\theta) = -\\text{ELBO} = \\underbrace{-\\mathbb{E}_{z \\sim q_\\phi(z|x)}\\left[ \\log p_\\theta(x|z) \\right]}_{\\mathcal{L}_{\\text{recon}}} + \\underbrace{D_{\\text{KL}}(q_\\phi(z|x) \\| p(z))}_{\\mathcal{L}_{\\text{KL}}}\n",
    "$$\n",
    "\n",
    "**Reconstruction Loss $\\mathcal{L}_{\\text{recon}}$**:\n",
    "- Measures how well the decoder reconstructs the input from sampled latent vectors\n",
    "- For Gaussian $p_\\theta(x|z)$ with fixed variance: equivalent to MSE loss\n",
    "- For Bernoulli $p_\\theta(x|z)$ (binary images): equivalent to BCE loss\n",
    "\n",
    "**KL Divergence Loss $\\mathcal{L}_{\\text{KL}}$**:\n",
    "- Regularizes the encoder to produce distributions close to the prior $p(z) = \\mathcal{N}(0, I)$\n",
    "- Prevents the encoder from collapsing to deterministic mappings\n",
    "- Ensures the latent space is smooth and continuous\n",
    "\n",
    "**Closed-form KL for Gaussians**:\n",
    "\n",
    "When $q_\\phi(z|x) = \\mathcal{N}(\\mu, \\text{diag}(\\sigma^2))$ and $p(z) = \\mathcal{N}(0, I)$, the KL divergence has a closed form:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(q_\\phi(z|x) \\| p(z)) = -\\frac{1}{2} \\sum_{j=1}^{D_z} \\left( 1 + \\log \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2 \\right)\n",
    "$$\n",
    "\n",
    "Or equivalently, using log-variance $\\gamma_j = \\log \\sigma_j^2$:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}} = -\\frac{1}{2} \\sum_{j=1}^{D_z} \\left( 1 + \\gamma_j - \\mu_j^2 - e^{\\gamma_j} \\right)\n",
    "$$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38dafbe-64ca-4b1d-92f4-ed3076c2a3ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### The Reparameterization Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c50da-a3aa-4c18-b119-b2589dcdd2f8",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "There's a problem: we need to backpropagate through the sampling operation $z \\sim q_\\phi(z|x)$, but sampling is **non-differentiable**.\n",
    "\n",
    "**The Solution: Reparameterization**\n",
    "\n",
    "Instead of sampling $z$ directly from $\\mathcal{N}(\\mu, \\sigma^2)$, we reparameterize as:\n",
    "\n",
    "$$\n",
    "z = \\mu + \\sigma \\odot \\epsilon, \\quad \\text{where } \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "Here $\\odot$ denotes element-wise multiplication.\n",
    "\n",
    "**Why this works:**\n",
    "- The randomness is now in $\\epsilon$, which doesn't depend on any parameters\n",
    "- $z$ is a deterministic function of $\\mu$, $\\sigma$, and $\\epsilon$\n",
    "- Gradients can flow through $\\mu$ and $\\sigma$ to the encoder\n",
    "\n",
    "**In PyTorch:**\n",
    "```python\n",
    "def reparameterize(self, mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)  # σ = exp(0.5 * log(σ²))\n",
    "    eps = torch.randn_like(std)     # ε ~ N(0, I)\n",
    "    return mu + eps * std           # z = μ + ε ⊙ σ\n",
    "```\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f2f947-199b-418d-bfd6-f3898ad58dc9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## VAE Architecture Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225ab789-96fc-4abe-88ca-9908ac17a307",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "The complete VAE architecture can be summarized as:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "x \\xrightarrow{\\text{Encoder}} (\\mu, \\log\\sigma^2) \\xrightarrow{\\text{Reparameterize}} z = \\mu + \\sigma \\odot \\epsilon \\xrightarrow{\\text{Decoder}} \\hat{x}\n",
    "}\n",
    "$$\n",
    "\n",
    "**Components:**\n",
    "\n",
    "| Component | Input | Output | Function |\n",
    "|-----------|-------|--------|----------|\n",
    "| Encoder $E_\\phi$ | $x \\in \\mathbb{R}^{D_x}$ | $\\mu, \\log\\sigma^2 \\in \\mathbb{R}^{D_z}$ | Learn approximate posterior |\n",
    "| Reparameterization | $\\mu, \\sigma, \\epsilon$ | $z \\in \\mathbb{R}^{D_z}$ | Enable backpropagation |\n",
    "| Decoder $D_\\theta$ | $z \\in \\mathbb{R}^{D_z}$ | $\\hat{x} \\in \\mathbb{R}^{D_x}$ | Generate reconstruction |\n",
    "\n",
    "**Training:**\n",
    "$$\n",
    "\\mathcal{L}_{\\text{VAE}} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\text{Reconstruction}} + \\underbrace{\\beta \\cdot D_{\\text{KL}}(q_\\phi(z|x) \\| p(z))}_{\\text{Regularization}}\n",
    "$$\n",
    "\n",
    "where $\\beta$ is a hyperparameter (often $\\beta = 1$, but $\\beta$-VAE uses different values).\n",
    "\n",
    "**Generation:**\n",
    "$$\n",
    "z \\sim \\mathcal{N}(0, I) \\xrightarrow{D_\\theta} \\hat{x}_{\\text{new}}\n",
    "$$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf85de3-bf36-40be-a2bc-8ef61491d6a5",
   "metadata": {},
   "source": [
    "## VAE Implementation in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a148c30-171a-4031-977c-0b6a97b69d5f",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "We now implement a convolutional VAE in PyTorch. The architecture uses:\n",
    "- **Encoder**: Convolutional layers to extract features, then linear layers to output $\\mu$ and $\\log\\sigma^2$\n",
    "- **Decoder**: Linear layer followed by transposed convolutions to reconstruct the image\n",
    "\n",
    "We'll design this for 28×28 grayscale images (MNIST/Fashion-MNIST).\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d7cdd-1fcc-45a2-b926-99fe6d297242",
   "metadata": {},
   "source": [
    "### Building the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec9460fd-9494-41c2-8923-d18f7cca6104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Variational Autoencoder.\n",
    "    \n",
    "    Architecture for 28x28 input:\n",
    "        Encoder: (1, 28, 28) -> Conv layers -> Flatten -> (μ, log σ²)\n",
    "        Decoder: z -> FC -> Unflatten -> ConvTranspose layers -> (1, 28, 28)\n",
    "    \n",
    "    The encoder outputs parameters of q(z|x) = N(μ, σ²I)\n",
    "    The decoder defines p(x|z)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim: int = 32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # ============ Encoder ============\n",
    "        # Convolutional layers: (1, 28, 28) -> (32, 4, 4)\n",
    "        self.encoder = nn.Sequential(\n",
    "            # (1, 28, 28) -> (8, 14, 14)\n",
    "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # (8, 14, 14) -> (16, 7, 7)\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # (16, 7, 7) -> (32, 4, 4)  (note: 7->4 with stride=2, padding=1)\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Flatten: (32, 4, 4) -> (512,)\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Latent space projections\n",
    "        # The flattened encoder output has size 32 * 4 * 4 = 512\n",
    "        self.fc_mu = nn.Linear(512, latent_dim)      # μ\n",
    "        self.fc_logvar = nn.Linear(512, latent_dim)  # log(σ²)\n",
    "        \n",
    "        # ============ Decoder ============\n",
    "        # Project from latent space back to feature maps\n",
    "        self.fc_decode = nn.Linear(latent_dim, 512)\n",
    "        \n",
    "        # Transposed convolutions: (32, 4, 4) -> (1, 28, 28)\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Unflatten: (512,) -> (32, 4, 4)\n",
    "            nn.Unflatten(1, (32, 4, 4)),\n",
    "            # (32, 4, 4) -> (16, 7, 7)\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=0),\n",
    "            nn.ReLU(),\n",
    "            # (16, 7, 7) -> (8, 14, 14)\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            # (8, 14, 14) -> (1, 28, 28)\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  # Output in [0, 1] for pixel values\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode input to distribution parameters.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (B, 1, 28, 28)\n",
    "        \n",
    "        Returns:\n",
    "            mu: Mean of q(z|x), shape (B, latent_dim)\n",
    "            logvar: Log-variance of q(z|x), shape (B, latent_dim)\n",
    "        \"\"\"\n",
    "        h = self.encoder(x)  # (B, 512)\n",
    "        mu = self.fc_mu(h)        # (B, latent_dim)\n",
    "        logvar = self.fc_logvar(h)  # (B, latent_dim)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: z = μ + σ ⊙ ε, where ε ~ N(0, I)\n",
    "        \n",
    "        This allows backpropagation through the sampling operation.\n",
    "        \n",
    "        Args:\n",
    "            mu: Mean of the latent distribution (B, latent_dim)\n",
    "            logvar: Log-variance of the latent distribution (B, latent_dim)\n",
    "        \n",
    "        Returns:\n",
    "            z: Sampled latent vector (B, latent_dim)\n",
    "        \"\"\"\n",
    "        # Compute standard deviation: σ = exp(0.5 * log(σ²))\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        \n",
    "        # Sample ε ~ N(0, I)\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        # z = μ + ε ⊙ σ\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decode latent vector to reconstruction.\n",
    "        \n",
    "        Args:\n",
    "            z: Latent vector of shape (B, latent_dim)\n",
    "        \n",
    "        Returns:\n",
    "            x_hat: Reconstructed image of shape (B, 1, 28, 28)\n",
    "        \"\"\"\n",
    "        h = self.fc_decode(z)  # (B, 512)\n",
    "        return self.decoder(h)  # (B, 1, 28, 28)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Full forward pass: encode -> reparameterize -> decode\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (B, 1, 28, 28)\n",
    "        \n",
    "        Returns:\n",
    "            x_hat: Reconstruction of shape (B, 1, 28, 28)\n",
    "            mu: Mean of q(z|x) for KL computation\n",
    "            logvar: Log-variance of q(z|x) for KL computation\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a89ff-5c0e-40ad-89dc-0937464dc06b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
